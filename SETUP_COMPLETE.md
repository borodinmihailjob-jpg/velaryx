# ‚úÖ AstroBot: Tailscale + Auto-Fallback Setup Complete

**–î–∞—Ç–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:** 2026-02-16
**–°—Ç–∞—Ç—É—Å:** üéâ –ü–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç!

---

## üéØ –ß—Ç–æ –±—ã–ª–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ

### 1. **Ollama Proxy** (`~/ollama-proxy/`)
- ‚úÖ FastAPI proxy —Å–µ—Ä–≤–µ—Ä —Å health checks
- ‚úÖ –ê–≤—Ç–æ–∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ launchd (PID: 47045)
- ‚úÖ –õ–æ–≥–∏: `~/ollama-proxy/proxy.log`
- ‚úÖ –ü–æ—Ä—Ç: 8888

### 2. **Tailscale Funnel**
- ‚úÖ –ü—É–±–ª–∏—á–Ω—ã–π HTTPS endpoint
- ‚úÖ URL: **https://macbook-pro.tailba5f18.ts.net**
- ‚úÖ –î–æ—Å—Ç—É–ø–µ–Ω –∏–∑ –ª—é–±–æ–π —Ç–æ—á–∫–∏ –º–∏—Ä–∞
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π SSL —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç

### 3. **Backend Auto-Fallback**
- ‚úÖ Primary: Ollama —á–µ—Ä–µ–∑ Tailscale
- ‚úÖ Fallback #1: OpenRouter (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏)
- ‚úÖ Fallback #2: Gemini
- ‚úÖ Timeout: 60 —Å–µ–∫—É–Ω–¥
- ‚úÖ –ü–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ fallback —Å–æ–±—ã—Ç–∏–π

### 4. **–¢–µ–∫—É—â–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (.env)**
```bash
LLM_PROVIDER=auto
OLLAMA_BASE_URL=https://macbook-pro.tailba5f18.ts.net
OLLAMA_TIMEOUT_SECONDS=60
OLLAMA_MODEL=qwen2.5:7b
```

---

## üìä –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã

```bash
# 1. Proxy –ª–æ–∫–∞–ª—å–Ω–æ
curl http://localhost:8888/health
# ‚Üí {"status":"healthy","ollama":"available","version":"0.16.1"}

# 2. Proxy —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç
curl https://macbook-pro.tailba5f18.ts.net/health
# ‚Üí {"status":"healthy","ollama":"available","version":"0.16.1"}

# 3. Backend API
curl http://localhost:8000/health
# ‚Üí {"ok": true, "timestamp": "..."}

# 4. LLM —Ç–µ—Å—Ç (–¥–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å AI interpretation)
curl http://localhost:8000/v1/tarot/draw -X POST \
  -H "Content-Type: application/json" \
  -H "X-TG-USER-ID: 123" \
  -d '{"spread_type":"three_card","question":"–¢–µ—Å—Ç"}'
# ‚Üí "llm_provider": "ollama:qwen2.5:7b"
```

---

## üöÄ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Telegram Bot   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Backend API    ‚îÇ  LLM_PROVIDER=auto
‚îÇ  (Docker)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îú‚îÄ PRIMARY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                      ‚îÇ
         ‚ñº                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Tailscale Funnel ‚îÇ   ‚îÇ  Mac (Ollama)     ‚îÇ
‚îÇ HTTPS            ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ  qwen2.5:7b       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îÇ (–µ—Å–ª–∏ Mac offline)
         ‚îÇ
         ‚îú‚îÄ FALLBACK #1 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ OpenRouter (–±–µ—Å–ø–ª–∞—Ç–Ω–æ)
         ‚îÇ
         ‚îî‚îÄ FALLBACK #2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Gemini
```

---

## üìù –ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã

### –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ Ollama Proxy
```bash
# –°—Ç–∞—Ç—É—Å
launchctl list | grep ollama-proxy

# –õ–æ–≥–∏ (—Ä–µ–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è)
tail -f ~/ollama-proxy/proxy.log

# –û—à–∏–±–∫–∏
tail -f ~/ollama-proxy/proxy.error.log

# –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å
launchctl unload ~/Library/LaunchAgents/com.astrobot.ollama-proxy.plist

# –ó–∞–ø—É—Å—Ç–∏—Ç—å
launchctl load ~/Library/LaunchAgents/com.astrobot.ollama-proxy.plist
```

### Tailscale Funnel
```bash
# –°—Ç–∞—Ç—É—Å
tailscale funnel status

# –í–∞—à –ø—É–±–ª–∏—á–Ω—ã–π URL
tailscale funnel status | grep https://

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
curl https://macbook-pro.tailba5f18.ts.net/health
```

### Backend
```bash
# –õ–æ–≥–∏ LLM –∑–∞–ø—Ä–æ—Å–æ–≤
docker compose logs -f api | grep LLM

# –¢–æ–ª—å–∫–æ fallback —Å–æ–±—ã—Ç–∏—è
docker compose logs -f api | grep fallback

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 100 –∑–∞–ø—Ä–æ—Å–æ–≤)
docker compose logs --tail=100 api | \
  grep "LLM success" | \
  sed -E 's/.*provider=([^ ]+).*/\1/' | \
  sort | uniq -c

# –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫
docker compose restart api
```

### –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
```bash
# –ü–æ–ª–Ω—ã–π —Ç–µ—Å—Ç —Å–∏—Å—Ç–µ–º—ã
./test-llm-fallback.sh

# –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç
curl http://localhost:8000/v1/tarot/draw -X POST \
  -H "Content-Type: application/json" \
  -H "X-TG-USER-ID: 123" \
  -d '{"spread_type":"three_card","question":"–¢–µ—Å—Ç"}'
```

---

## üîß Troubleshooting

### Proxy –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
```bash
# 1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ –∑–∞–ø—É—â–µ–Ω
ps aux | grep "python.*main.py"

# 2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏
tail -20 ~/ollama-proxy/proxy.error.log

# 3. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å
launchctl unload ~/Library/LaunchAgents/com.astrobot.ollama-proxy.plist
launchctl load ~/Library/LaunchAgents/com.astrobot.ollama-proxy.plist
```

### Tailscale Funnel –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
```bash
# 1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å Tailscale
tailscale status

# 2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å Funnel
tailscale funnel status

# 3. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å Funnel
tailscale funnel off
tailscale funnel 8888
```

### Backend –∏—Å–ø–æ–ª—å–∑—É–µ—Ç fallback –≤–º–µ—Å—Ç–æ Ollama
```bash
# 1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å .env
grep OLLAMA_BASE_URL .env
grep OLLAMA_TIMEOUT .env

# 2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ URL –¥–æ—Å—Ç—É–ø–µ–Ω
curl https://macbook-pro.tailba5f18.ts.net/health

# 3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏
docker compose logs api --tail=50 | grep -E "(LLM|Ollama)"

# 4. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é
docker compose down && docker compose up -d
```

### Ollama –º–µ–¥–ª–µ–Ω–Ω–æ –æ—Ç–≤–µ—á–∞–µ—Ç
```bash
# 1. –ü—Ä–æ–≥—Ä–µ—Ç—å –º–æ–¥–µ–ª—å
curl -s http://localhost:11434/api/generate -d '{
  "model": "qwen2.5:7b",
  "prompt": "–ü—Ä–∏–≤–µ—Ç",
  "stream": false
}'

# 2. –£–≤–µ–ª–∏—á–∏—Ç—å timeout –≤ .env
OLLAMA_TIMEOUT_SECONDS=90

# 3. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å backend
docker compose restart api
```

---

## üéÅ –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –î–æ–±–∞–≤–∏—Ç—å fallback –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã

–î–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –¥–æ–±–∞–≤—å—Ç–µ API –∫–ª—é—á–∏ –æ–±–ª–∞—á–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤:

### OpenRouter (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏)
1. –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è: https://openrouter.ai
2. API Key: https://openrouter.ai/keys
3. Privacy Settings: https://openrouter.ai/settings/privacy
   - ‚úÖ –í–∫–ª—é—á–∏—Ç—å: "Allow free models to use my data for training"
4. –î–æ–±–∞–≤–∏—Ç—å –≤ `.env`:
   ```bash
   OPENROUTER_API_KEY=sk-or-v1-...
   ```

### Gemini (Google AI)
1. API Key: https://makersuite.google.com/app/apikey
2. –õ–∏–º–∏—Ç—ã: 15 requests/minute (–±–µ—Å–ø–ª–∞—Ç–Ω–æ)
3. –î–æ–±–∞–≤–∏—Ç—å –≤ `.env`:
   ```bash
   GEMINI_API_KEY=...
   ```

–ü–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∫–ª—é—á–µ–π:
```bash
docker compose restart api
```

---

## üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

- [NEXT_STEPS.md](NEXT_STEPS.md) - –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è
- [TAILSCALE_SETUP.md](TAILSCALE_SETUP.md) - –î–µ—Ç–∞–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞
- [ollama-proxy/README.md](ollama-proxy/README.md) - –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è proxy
- [ollama-proxy/COMMANDS.md](ollama-proxy/COMMANDS.md) - –®–ø–∞—Ä–≥–∞–ª–∫–∞ –∫–æ–º–∞–Ω–¥
- [test-llm-fallback.sh](test-llm-fallback.sh) - –¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç

---

## ‚úÖ Checklist –∑–∞–≤–µ—Ä—à–µ–Ω–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏

- [x] Ollama —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç
- [x] –ú–æ–¥–µ–ª—å qwen2.5:7b –∑–∞–≥—Ä—É–∂–µ–Ω–∞
- [x] Ollama Proxy —Å–æ–∑–¥–∞–Ω –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω
- [x] Python –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã
- [x] launchd service –Ω–∞—Å—Ç—Ä–æ–µ–Ω
- [x] Tailscale —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
- [x] Tailscale Funnel –≤–∫–ª—é—á–µ–Ω
- [x] –ü—É–±–ª–∏—á–Ω—ã–π URL –ø–æ–ª—É—á–µ–Ω
- [x] .env –æ–±–Ω–æ–≤–ª–µ–Ω —Å Tailscale URL
- [x] Backend –Ω–∞—Å—Ç—Ä–æ–µ–Ω –Ω–∞ auto-fallback
- [x] Timeout —É–≤–µ–ª–∏—á–µ–Ω –¥–æ 60 —Å–µ–∫—É–Ω–¥
- [x] –°–∏—Å—Ç–µ–º–∞ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞
- [x] LLM —Ä–∞–±–æ—Ç–∞–µ—Ç —á–µ—Ä–µ–∑ Tailscale

---

## üéØ –ß—Ç–æ –¥–∞–ª—å—à–µ?

1. **–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ:** –î–æ–±–∞–≤–∏—Ç—å API –∫–ª—é—á–∏ –¥–ª—è OpenRouter/Gemini (—Å–º. –≤—ã—à–µ)
2. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:** –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø—Ä–æ–≤–µ—Ä—è—Ç—å –ª–æ–≥–∏ `docker compose logs api | grep LLM`
3. **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:** –û—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —Å–∫–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–¥–µ—Ç —á–µ—Ä–µ–∑ Ollama vs fallback
4. **Deploy –Ω–∞ Render:** –ö–æ–≥–¥–∞ –≥–æ—Ç–æ–≤ - –∑–∞–¥–µ–ø–ª–æ–∏—Ç—å —Å —ç—Ç–∏–º–∏ –∂–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏

---

## üí° –ü–æ–ª–µ–∑–Ω—ã–µ –∞–ª–∏–∞—Å—ã –¥–ª—è ~/.zshrc

–î–æ–±–∞–≤—å—Ç–µ –≤ `~/.zshrc` –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞:

```bash
# Ollama Proxy
alias proxy-logs='tail -f ~/ollama-proxy/proxy.log'
alias proxy-health='curl -s http://localhost:8888/health | jq'
alias proxy-restart='launchctl unload ~/Library/LaunchAgents/com.astrobot.ollama-proxy.plist && launchctl load ~/Library/LaunchAgents/com.astrobot.ollama-proxy.plist'

# Tailscale
alias ts-status='tailscale status'
alias ts-url='tailscale funnel status | grep https://'

# AstroBot
alias astro-logs='docker compose logs -f api | grep LLM'
alias astro-test='./test-llm-fallback.sh'
alias astro-stats='docker compose logs --tail=100 api | grep "LLM success" | sed -E "s/.*provider=([^ ]+).*/\1/" | sort | uniq -c'

# –ü–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è:
source ~/.zshrc
```

---

**üéâ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –í—Å—ë —Ä–∞–±–æ—Ç–∞–µ—Ç!**

–í–∞—à–∞ LLM —Ç–µ–ø–µ—Ä—å –¥–æ—Å—Ç—É–ø–Ω–∞ –∏–∑ –ª—é–±–æ–π —Ç–æ—á–∫–∏ –º–∏—Ä–∞ —á–µ—Ä–µ–∑ HTTPS —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback –Ω–∞ –æ–±–ª–∞—á–Ω—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã. ‚ú®
