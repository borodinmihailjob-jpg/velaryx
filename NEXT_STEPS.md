# üéØ –ß—Ç–æ –æ—Å—Ç–∞–ª–æ—Å—å —Å–¥–µ–ª–∞—Ç—å (—Ç—Ä–µ–±—É–µ—Ç—Å—è –≤–∞—à–µ —É—á–∞—Å—Ç–∏–µ)

–Ø –Ω–∞—Å—Ç—Ä–æ–∏–ª –≤—Å—ë —á—Ç–æ –º–æ–≥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏! –í–æ—Ç —á—Ç–æ –æ—Å—Ç–∞–ª–æ—Å—å:

---

## ‚úÖ –ß—Ç–æ —É–∂–µ –≥–æ—Ç–æ–≤–æ

1. ‚úÖ **~/ollama-proxy/** - —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–æ–∑–¥–∞–Ω–∞
2. ‚úÖ **Python –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏** - —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã (fastapi, uvicorn, httpx)
3. ‚úÖ **launchd plist** - —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω –≤ `~/Library/LaunchAgents/`
4. ‚úÖ **.env** - –æ–±–Ω–æ–≤–ª–µ–Ω —Å `LLM_PROVIDER=auto` –∏ `OLLAMA_TIMEOUT_SECONDS=10`

---

## üìã –û—Å—Ç–∞–ª–æ—Å—å 3 —à–∞–≥–∞ (10-15 –º–∏–Ω—É—Ç)

### –®–∞–≥ 1: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å Tailscale (5 –º–∏–Ω—É—Ç)

```bash
# 1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Tailscale
brew install tailscale

# 2. –ó–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Ä–≤–∏—Å
sudo brew services start tailscale

# 3. –ê–≤—Ç–æ—Ä–∏–∑–æ–≤–∞—Ç—å—Å—è (–æ—Ç–∫—Ä–æ–µ—Ç—Å—è –±—Ä–∞—É–∑–µ—Ä)
sudo tailscale up

# 4. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç
tailscale status
```

**–ì–æ—Ç–æ–≤–æ!** Tailscale —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.

---

### –®–∞–≥ 2: –ó–∞–ø—É—Å—Ç–∏—Ç—å proxy –∏ –≤–∫–ª—é—á–∏—Ç—å Funnel (5 –º–∏–Ω—É—Ç)

```bash
# 1. –ó–∞–ø—É—Å—Ç–∏—Ç—å proxy –≤—Ä—É—á–Ω—É—é (–¥–ª—è —Ç–µ—Å—Ç–∞)
cd ~/ollama-proxy
~/ollama-proxy/venv/bin/python main.py &

# –í—ã –¥–æ–ª–∂–Ω—ã —É–≤–∏–¥–µ—Ç—å:
# INFO: Started server process [12345]
# INFO: Waiting for application startup.
# INFO: Application startup complete.
# INFO: Uvicorn running on http://0.0.0.0:8888

# 2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ proxy —Ä–∞–±–æ—Ç–∞–µ—Ç
curl http://localhost:8888/health

# –î–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å:
# {"status":"healthy","ollama":"available"}

# 3. –í–∫–ª—é—á–∏—Ç—å Tailscale Funnel
tailscale funnel 8888

# –í—ã –ø–æ–ª—É—á–∏—Ç–µ URL —Ç–∏–ø–∞:
# Available within your tailnet:
#   https://your-macbook.tail1234.ts.net/
#
# Available on the internet:
#   https://your-macbook.tail1234.ts.net/

# 4. –í–ê–ñ–ù–û: –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç URL!
echo "–ú–æ–π Tailscale URL:"
tailscale funnel status | grep "https://"

# 5. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç
curl https://your-macbook.tail1234.ts.net/health
```

**‚úÖ –ï—Å–ª–∏ –≤–µ—Ä–Ω—É–ª `{"status":"healthy"}` - Funnel —Ä–∞–±–æ—Ç–∞–µ—Ç!**

---

### –®–∞–≥ 3: –ó–∞–≥—Ä—É–∑–∏—Ç—å launchd service (2 –º–∏–Ω—É—Ç—ã)

```bash
# 1. –ó–∞–≥—Ä—É–∑–∏—Ç—å service (–∞–≤—Ç–æ–∑–∞–ø—É—Å–∫ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ Mac)
launchctl load ~/Library/LaunchAgents/com.astrobot.ollama-proxy.plist

# 2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ –∑–∞–ø—É—Å—Ç–∏–ª—Å—è
launchctl list | grep ollama-proxy

# –î–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å:
# 12345  0  com.astrobot.ollama-proxy

# 3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏
tail -f ~/ollama-proxy/proxy.log

# –î–æ–ª–∂–Ω—ã —É–≤–∏–¥–µ—Ç—å:
# INFO: Started server process
# INFO: Application startup complete.
```

**‚úÖ Proxy —Ç–µ–ø–µ—Ä—å –±—É–¥–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–ø—É—Å–∫–∞—Ç—å—Å—è!**

---

### –®–∞–≥ 4: –û–±–Ω–æ–≤–∏—Ç—å .env —Å Tailscale URL (1 –º–∏–Ω—É—Ç–∞)

–û—Ç–∫—Ä–æ–π—Ç–µ `.env` –∏ –∑–∞–º–µ–Ω–∏—Ç–µ:

```bash
# –ë—ã–ª–æ:
OLLAMA_BASE_URL=http://host.docker.internal:11434

# –°—Ç–∞–ª–æ (–≤—Å—Ç–∞–≤—å—Ç–µ –≤–∞—à URL –∏–∑ –®–∞–≥–∞ 2):
OLLAMA_BASE_URL=https://your-macbook.tail1234.ts.net
```

**–ò–ª–∏ —á–µ—Ä–µ–∑ –∫–æ–º–∞–Ω–¥–Ω—É—é —Å—Ç—Ä–æ–∫—É:**

```bash
# –ó–∞–º–µ–Ω–∏—Ç–µ YOUR_URL –Ω–∞ –≤–∞—à —Ä–µ–∞–ª—å–Ω—ã–π URL
sed -i '' 's|OLLAMA_BASE_URL=.*|OLLAMA_BASE_URL=https://YOUR_URL.ts.net|' .env
```

---

### –®–∞–≥ 5: –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å (3 –º–∏–Ω—É—Ç—ã)

```bash
# 1. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å backend
docker compose down
docker compose up --build -d

# 2. –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç
./test-llm-fallback.sh

# 3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏
docker compose logs api | grep LLM

# –î–æ–ª–∂–Ω—ã —É–≤–∏–¥–µ—Ç—å:
# LLM request | provider=auto (ollama‚Üíopenrouter‚Üígemini)
# LLM success | provider=ollama | time=2.34s
```

**‚úÖ –ï—Å–ª–∏ –≤–∏–¥–∏—Ç–µ "provider=ollama" - –≤—Å—ë —Ä–∞–±–æ—Ç–∞–µ—Ç!**

---

## üéÅ –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –ü–æ–ª—É—á–∏—Ç—å API –∫–ª—é—á–∏ –¥–ª—è fallback

–ï—Å–ª–∏ Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, backend –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—Å—è –Ω–∞:

### OpenRouter (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏)

1. –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è: https://openrouter.ai
2. –ü–æ–ª—É—á–∏—Ç—å –∫–ª—é—á: https://openrouter.ai/keys
3. –ù–∞—Å—Ç—Ä–æ–∏—Ç—å privacy: https://openrouter.ai/settings/privacy
   - ‚úÖ –í–∫–ª—é—á–∏—Ç—å: "Allow free models to use my data for training"
4. –î–æ–±–∞–≤–∏—Ç—å –≤ `.env`:
   ```bash
   OPENROUTER_API_KEY=sk-or-v1-...
   ```

### Gemini (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

1. –ü–æ–ª—É—á–∏—Ç—å –∫–ª—é—á: https://makersuite.google.com/app/apikey
2. –î–æ–±–∞–≤–∏—Ç—å –≤ `.env`:
   ```bash
   GEMINI_API_KEY=...
   ```

---

## üß™ –¢–µ—Å—Ç fallback –º–µ—Ö–∞–Ω–∏–∑–º–∞

### –¢–µ—Å—Ç 1: Ollama —Ä–∞–±–æ—Ç–∞–µ—Ç
```bash
docker compose logs api | grep "LLM success"
# –û–∂–∏–¥–∞–µ–º: provider=ollama
```

### –¢–µ—Å—Ç 2: Ollama offline (fallback)
```bash
# –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å proxy
launchctl unload ~/Library/LaunchAgents/com.astrobot.ollama-proxy.plist

# –°–¥–µ–ª–∞—Ç—å –∑–∞–ø—Ä–æ—Å
curl http://localhost:8000/v1/tarot/draw -X POST \
  -H "Content-Type: application/json" \
  -H "X-TG-USER-ID: 123" \
  -d '{"spread_type":"three_card","question":"test"}'

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏
docker compose logs api | grep "LLM"

# –û–∂–∏–¥–∞–µ–º:
# Ollama failed, trying OpenRouter fallback...
# LLM success | provider=openrouter (fallback)

# –ó–∞–ø—É—Å—Ç–∏—Ç—å proxy —Å–Ω–æ–≤–∞
launchctl load ~/Library/LaunchAgents/com.astrobot.ollama-proxy.plist
```

---

## üìö –ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã

```bash
# === –°—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–æ–≤ ===
curl http://localhost:8888/health              # Proxy –ª–æ–∫–∞–ª—å–Ω–æ
curl https://YOUR_URL.ts.net/health            # Proxy —á–µ—Ä–µ–∑ Tailscale
tailscale status                               # Tailscale —Å—Ç–∞—Ç—É—Å
tailscale funnel status                        # Funnel —Å—Ç–∞—Ç—É—Å
docker compose logs api | grep LLM             # Backend LLM –ª–æ–≥–∏

# === –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ proxy ===
launchctl unload ~/Library/LaunchAgents/com.astrobot.ollama-proxy.plist  # Stop
launchctl load ~/Library/LaunchAgents/com.astrobot.ollama-proxy.plist    # Start
tail -f ~/ollama-proxy/proxy.log               # –õ–æ–≥–∏

# === –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ===
./test-llm-fallback.sh                         # –ê–≤—Ç–æ—Ç–µ—Å—Ç
docker compose logs -f api | grep fallback     # –û—Ç—Å–ª–µ–¥–∏—Ç—å fallback —Å–æ–±—ã—Ç–∏—è
```

---

## üéØ –ö—Ä–∞—Ç–∫–∏–π —á–µ–∫–ª–∏—Å—Ç

- [ ] `brew install tailscale`
- [ ] `sudo tailscale up`
- [ ] `cd ~/ollama-proxy && ~/ollama-proxy/venv/bin/python main.py &`
- [ ] `tailscale funnel 8888`
- [ ] –°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å URL –∏–∑ `tailscale funnel status`
- [ ] `launchctl load ~/Library/LaunchAgents/com.astrobot.ollama-proxy.plist`
- [ ] –û–±–Ω–æ–≤–∏—Ç—å `OLLAMA_BASE_URL` –≤ `.env` –Ω–∞ Tailscale URL
- [ ] `docker compose up --build`
- [ ] `./test-llm-fallback.sh`

---

## üìñ –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

–ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –Ω–µ —è—Å–Ω–æ:
- [TAILSCALE_SETUP.md](TAILSCALE_SETUP.md) - –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è
- [ollama-proxy/README.md](ollama-proxy/README.md) - –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è proxy
- [ollama-proxy/COMMANDS.md](ollama-proxy/COMMANDS.md) - –®–ø–∞—Ä–≥–∞–ª–∫–∞ –∫–æ–º–∞–Ω–¥

---

## üÜò –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç

### Proxy –Ω–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è
```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏ –æ—à–∏–±–æ–∫
cat ~/ollama-proxy/proxy.error.log

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ Python –∏ –ø–∞–∫–µ—Ç—ã —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã
~/ollama-proxy/venv/bin/python --version
~/ollama-proxy/venv/bin/pip list | grep fastapi
```

### Tailscale –Ω–µ –∞–≤—Ç–æ—Ä–∏–∑—É–µ—Ç—Å—è
```bash
# –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å–Ω–æ–≤–∞
sudo tailscale down
sudo tailscale up

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å
tailscale status
```

### Backend –Ω–µ –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫ Ollama
```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ URL –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π
grep OLLAMA_BASE_URL .env

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ URL –æ—Ç–≤–µ—á–∞–µ—Ç
curl $(grep OLLAMA_BASE_URL .env | cut -d= -f2)/health

# –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å backend
docker compose restart api
```

---

**–ì–æ—Ç–æ–≤–æ!** –ü–æ—Å–ª–µ —ç—Ç–∏—Ö —à–∞–≥–æ–≤ —É –≤–∞—Å –±—É–¥–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ä–∞–±–æ—á–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å auto-fallback üöÄ
